## When not to use AI.


A risk based approach to using or not using AI. And why we still need people.

The progress of Artificial Intelligence (AI) has been amazing the last couple of years. However, AI's still make 

# Some examples of false use of AI.

- Tesla sees the moon
- Self driving taxi's
- Example of Dutch man

Gary Marcus is a renowned AI scientist who is now the most fierce criticaster of the current AI-systems. He has written a (notorious) book called 'Rebooting AI where he explains his problems he sees 

In short summary Marcus has three criticisms of current AI systems:

- Brittle
- Opague
- 


Is Marcus rigth? Well, many other leading AI researchers (eg. Hinton, LeCunn) claim he is not. But given the examples above, 

# A risk based approach to using or not using AI's. 

So when should we or should we not use current AI systems? Well, if the risk of a failure does not outweigh the benefits. 

So what is risk? Risk can be defined as follows:

Risk = Chance * Impact

So a low chance with a very high impact, can still be a large risk. Take for example the self driving car: even if there is a low chance it will hit a child, the death of the child is a very high impact, resulting in a high risk.

Are there low impact AI's? Yes, there are many, such as OCR or translating simple text. 

#Human Centered AI?

Does that mean that we should not use AI at all in these situations? Well, maybe 'Human Centered AI' could be a practical way forward. In this approach the AI supports a human. For example, it's an AI that detects a spot on a lung photo, but ultimately it is the medical doctor that takes the decision whether the patient has lungcancer or not. 


#